\documentclass[a4paper,12pt]{report}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{url}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{titlesec}
\usepackage{cite}
\usepackage{booktabs}

% Page Geometry
\geometry{a4paper,left=25mm,right=25mm,top=30mm,bottom=30mm}

% Chapter Title Formatting
\titleformat{\chapter}[display]
  {\normalfont\huge\bfseries\centering}
  {\chaptertitlename\ \thechapter}{20pt}{\Huge}

% Code Snippet Configuration
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\begin{document}

% --- Title Page ---
\thispagestyle{empty}
\begin{center}
  \Huge{\textsc{Design and Implementation of a Memory Management Unit and Cache Controller in RTL}}
\end{center}
%\vspace{0.5cm}
\begin{center}
  \large{(Monsoon 2025)}
\end{center}
\vspace{0.5cm}
\begin{center}
  \large{Submitted by}
\end{center}
\vspace{0.25cm}
\begin{center}
  \large{Kalrav Mathur (2210110338)}\\
  \vspace{0.25cm}
  \large{Akhil Sriram (2210110134)}
\end{center}
\vspace{0.25cm}
\begin{center}
  \large{Under Supervision\\ of}
\end{center}
\begin{center}
  \large{Dr. Venkatnarayan Hariharan}\\
  \large{Department of Electrical Engineering}\\
\end{center}
\vspace{0.75cm}
\begin{figure}[htbp]
  \centering
  % Ensure 'logo.jpg' exists or comment out
  \includegraphics[width=4in]{logo.jpg}
\end{figure}
\pagebreak

% --- Abstract ---
\begin{center}
  \huge{\textsc{Abstract}}
\end{center}
This project presents the complete Register Transfer Level (RTL) design, implementation, and verification of a memory management subsystem, comprising a Translation Lookaside Buffer (TLB)-based Memory Management Unit (MMU) and a 2-Way Set-Associative Cache Controller. The system is engineered to address the ``memory wall'' bottleneck by providing high-speed virtual-to-physical address translation and efficient data caching. The design is implemented in Verilog HDL, emphasizing modularity to support future multi-level hierarchies. Key contributions include a robust Finite State Machine (FSM) for cache control, a write-through mechanism that updates both cache and main memory simultaneously to ensure strong consistency, and a novel script-driven verification environment. This environment parses human-readable instruction files (e.g., \texttt{R 0x1000}) to drive simulation scenarios, bridging the gap between architectural intent and RTL verification. Extensive simulations, including corner-case testing of TLB replacement policies and cache eviction scenarios, validate the correctness of the design. The project culminates in a synthesizable hardware description suitable for ASIC implementation flow.

\pagebreak
\tableofcontents
\pagebreak
\listoffigures
\pagebreak
\listoftables
\pagebreak

% --- Chapter 1: Introduction ---
\chapter{Introduction}
In modern high-performance computing, the performance disparity between high-frequency processors and high-latency main memory (DRAM)—often referred to as the ``Von Neumann bottleneck'' or the ``memory wall''—necessitates sophisticated memory hierarchies. This project focuses on the hardware implementation of two critical components designed to bridge this gap: the Memory Management Unit (MMU) for virtual memory support and the Cache Controller for latency reduction.

By organizing memory into a hierarchy of increasingly faster but smaller storage units closer to the processor, systems can provide the illusion of a single, large memory that operates at near-processor speeds. The cache controller manages this hierarchy by exploiting the principles of locality—both spatial and temporal—to keep frequently accessed data readily available. Simultaneously, the MMU enables efficient memory virtualization, allowing multiple processes to share physical memory securely and seamlessly.

\section{Motivation}
The implementation of a cache controller from scratch provides invaluable insights into micro-architectural trade-offs. Modern processors spend a significant portion of their die area on cache structures to hide the 100+ cycle latency of DRAM access. By developing the RTL for tag comparison, eviction, and write policies, we gain a cycle-accurate understanding of these latency-hiding mechanisms.
Furthermore, this project emphasizes a modular design approach, particularly for the MMU and Cache Controller. Such modularity is highly valued in computer architecture research, as it allows architects to benchmark and test system performance under various configurations (e.g., swapping replacement policies or changing associativity) without redesigning the entire system.

\section{Project Objectives}
The core objective is to create a functional, synthesizable, and modular RTL design of an MMU and a cache controller. The design focuses on correctness and cycle-accurate behavior.
Additionally, a key objective is to ensure modularity in the design. This modularity allows individual components, such as the address decomposition, cache replacement and write logic, to be easily modified without redesigning the entire system. This is crucial for performance benchmarking and testing by architects, who often need to evaluate different configurations and policies.

\subsection{Memory Management Unit (MMU) Scope}
The MMU design focuses on address translation and protection.
\begin{itemize}
  \item \textbf{Translation Logic:} Implements a TLB buffer to map 32-bit virtual addresses to 32-bit physical addresses using 4KB paging.
  \item \textbf{Miss Handling:} Detects TLB misses and asserts a \texttt{ptw\_miss\_detected} signal to freeze the CPU pipeline while a Page Table Walk occurs.
  \item \textbf{Replacement Policy:} Implements a FIFO replacement algorithm for the fully associative TLB to handle capacity misses efficiently.
\end{itemize}

\subsection{Cache Controller Scope}
The cache controller manages an 8KB, 2-way set-associative cache.
\begin{itemize}
  \item \textbf{FSM Control:} A deterministic 7-state FSM handles the complexity of hit detection, miss fetching, and refill operations.
  \item \textbf{Write Policy:} Implements a \textbf{Write-Through} policy. On a Write Hit, data is updated in \textit{both} the L1 Cache and Main Memory simultaneously.
  \item \textbf{Replacement Policy:} Utilizes a 1-bit Least Recently Used (LRU) policy for handling cache evictions efficiently.
\end{itemize}

\section{Methodology}
Our design methodology follows a rigorous ASIC design flow.
\begin{enumerate}
  \item \textbf{Architectural Modeling:} Defining state elements (SRAMs, Tag Stores) and control logic based on theoretical models.
  \item \textbf{RTL Design Entry:} Implementing modules in Verilog HDL, ensuring code is synthesizable (avoiding non-synthesizable constructs like initial blocks in logic).
  \item \textbf{Functional Verification:} Developing a layered testbench environment using Synopsys VCS. This includes unit testing for MMU/Cache separately and integration testing.
  \item \textbf{Debugging and Analysis:} Utilizing Synopsys Verdi for waveform analysis, signal tracing, and Finite State Machine coverage visualization.
\end{enumerate}

% --- Chapter 2: Literature Survey ---
\chapter{Literature Survey}

\section{Foundational Concepts in Computer Architecture}
A review of foundational literature was conducted to establish the theoretical basis for our design. Textbooks such as ``Computer Architecture: A Quantitative Approach'' by Hennessy and Patterson \cite{hennessy} and ``Computer Architecture and Organization'' by Smruti Sarangi \cite{sarangi2017computer} provided comprehensive insights into memory hierarchies, TLB associativity trade-offs, and caching principles.

For practical implementation strategies, we analyzed the open-source Bluespec implementation of the SHAKTI processor's cache controller \cite{shakti_cache}. Additionally, educational resources from GeeksforGeeks and Branch Education were consulted to reinforce concepts regarding VIPT vs PIPT caches and virtual memory page faults \cite{gfg_cache, branch_edu_cache, tech_nikola, gfg_vipt}.

\begin{table}[ht]
  \centering
  \caption{System Configuration Parameters}
  \vspace{0.2cm}
  \label{t1}
  \begin{tabular}{|l|l|l|l|}
    \hline
    \textbf{Parameter}    & \textbf{Value} & \textbf{Parameter}     & \textbf{Value} \\ \hline
    Virtual Address Width & 32 bits        & Physical Address Width & 32 bits        \\ \hline
    Cache Size            & 8 KB           & Associativity          & 2-way          \\ \hline
    Block Size            & 64 Bytes       & Write Policy           & Write-Through  \\ \hline
    Page Size             & 4 KB           & Replacement Policy     & 1-bit LRU      \\ \hline
  \end{tabular}
\end{table}

% --- Chapter 3: Work Done (Pre-Midsem) ---
\chapter{Work Done (Pre-Midsem)}

This chapter outlines the foundational work completed during the first phase of the project, focusing on architectural planning, theoretical modeling, and the initial design of the Memory Management Unit.

\section{System Architecture and Pipeline Design}
The primary achievement of the pre-midsem phase was the definition of the system architecture. The system follows a strict pipeline where the CPU issues a Virtual Address (VA), the MMU translates this to a Physical Address (PA), and the Cache Controller utilizes the PA to index into SRAMs.

\begin{figure}[H]
  \centering
  % Use the renamed image
  \includegraphics[width=1.0\textwidth]{overall_system.png}
  \caption{Overall System Architecture connecting CPU, MMU, Cache Controller, and Memory.}
  \label{fig:overall_arch}
\end{figure}

We established the communication protocols between modules:
\begin{itemize}
  \item \textbf{CPU-MMU Interface:} Includes VA, valid signal, and a stall signal to freeze the CPU during translation misses.
  \item \textbf{MMU-Cache Interface:} Passes the translated PA.
  \item \textbf{Cache-Cache-Memory Interface:} Handles 512-bit block transfers for cache refills and reads.
  \item \textbf{Cache-Main-Memory Interface:} Handles 32-bit word reads and writes for cache misses.
\end{itemize}

\section{Foundational Memory Management Formulas}
We derived the key addressing bit configuration required to implement the hardware logic.

\subsection{Cache Address Decomposition}
The 32-bit Physical Address is decomposed as follows, based on the 8KB size and 64B blocks:
\begin{equation}
  \text{Offset} = \log_2(64) = 6 \text{ bits}
\end{equation}
\begin{equation}
  \text{Index} = \log_2\left(\frac{8192}{64 \times 2}\right) = 6 \text{ bits}
\end{equation}
\begin{equation}
  \text{Tag} = 32 - 6 - 6 = 20 \text{ bits}
\end{equation}

\subsection{MMU Address Decomposition}
For virtual-to-physical translation, the MMU splits the address based on the 4KB page size.
\begin{equation}
  \text{Page\_Offset\_bits} = \log_2(4096) = 12 \text{ bits}
\end{equation}
\begin{equation}
  \text{VPN\_bits} = 32 - 12 = 20 \text{ bits}
\end{equation}

\section{MMU Conceptual Design}
We finalized the design for the MMU, choosing a fully associative Translation Lookaside Buffer (TLB) to maximize hit rates for small entry counts. The design includes:
\begin{itemize}
  \item \textbf{Content Addressable Memory (CAM):} For parallel tag comparison.
  \item \textbf{Replacement Logic:} A FIFO pointer to manage evictions.
  \item \textbf{Page Table Walk Interface:} A conceptual interface to handle misses by communicating with main memory (simulated in testbench).
\end{itemize}

% --- Chapter 4: Work Done (Post-Midsem) ---
\chapter{Work Done (Post-Midsem)}

The post-midsem phase focused on the concrete RTL implementation, coding, unit testing, and system integration. This chapter details the Verilog development and verification results.

\section{RTL Code Development}
We implemented the modules in Verilog HDL.

\subsection{MMU and TLB Implementation}
The MMU functionality is split into parameter definitions and the TLB logic.

\textbf{MMU Parameters (\texttt{mmu\_params.v}):}
This file defines the configurable bit-widths for the address translation, ensuring the design can be easily resized.
\begin{lstlisting}[language=Verilog, caption=MMU Parameter Definitions]
`ifndef MMU_PARAMS_V
`define MMU_PARAMS_V

// Address Sizes (configurable)
`define ADDR_WIDTH 32
`define PAGE_SIZE 4096

`define OFFSET_BITS $clog2(`PAGE_SIZE)

`define VPN_WIDTH (`ADDR_WIDTH - `OFFSET_BITS)
`define PFN_WIDTH (`ADDR_WIDTH - `OFFSET_BITS)

// --- TLB Configuration ---
`define TLB_ENTRIES 4
`define TLB_PER_BITS $clog2(`TLB_ENTRIES)

// --- MMU Status Codes ---
`define STATUS_OK 2'b10

`endif  // MMU_PARAMS_V
\end{lstlisting}

\textbf{TLB Logic (\texttt{tlb\_simple.v}):}
The TLB implements a fully associative lookup using a for-loop, which synthesizes into parallel comparators. The refill logic uses a \texttt{replace\_ptr} to implement a FIFO replacement policy.
\begin{lstlisting}[language=Verilog, caption=TLB Lookup and Refill Logic]
module tlb_simple (
    // ... ports ...
    input wire [`VPN_WIDTH-1:0] lookup_vpn,
    output reg                lookup_hit,
    output reg [`PFN_WIDTH-1:0] lookup_pfn,
    // ...
);
    // Internal storage
    reg [`VPN_WIDTH-1:0] tag_mem [0:`TLB_ENTRIES-1];
    reg [`PFN_WIDTH-1:0] data_mem [0:`TLB_ENTRIES-1];
    // ...

    // 1. Combinational Lookup Logic (Fully Associative)
    always @(*) begin
        lookup_hit = 1'b0;
        lookup_pfn = {`PFN_WIDTH{1'b0}};

        for (i = 0; i < `TLB_ENTRIES; i = i + 1) begin
            if (valid_mem[i] && (tag_mem[i] == lookup_vpn)) begin
                lookup_hit = 1'b1;
                lookup_pfn = data_mem[i];
            end
        end
    end

    // 2. Sequential Update Logic (Refill and Pointer Update)
    always @(posedge clk or negedge rst_n) begin
        if (!rst_n) begin
            replace_ptr <= {`TLB_PER_BITS{1'b0}};
            // ... reset logic ...
        end else begin
            if (refill_en) begin
                tag_mem[replace_ptr]   <= refill_vpn;
                data_mem[replace_ptr]  <= refill_pfn;
                valid_mem[replace_ptr] <= 1'b1;
                
                replace_ptr <= replace_ptr + 1'b1; // FIFO
            end
        end
    end
endmodule
\end{lstlisting}
\subsection{Cache Memory Modeling}
The L1 Cache memory (\texttt{cache\_mem.v}) simulates the data storage array. It is modeled as two separate banks (\texttt{way0} and \texttt{way1}) to support the 2-way set-associative structure. Each entry stores a full 512-bit (64-byte) cache block.

\begin{lstlisting}[language=Verilog, caption=Cache Memory Model]
// Storage: 64 sets, 2 ways
// Cache block: 512 bits 
reg [511:0] way0[0:63];
reg [511:0] way1[0:63];

// --- Read Operation (Combinational/Asynchronous) ---
// This mimics the behavior of selecting the correct way based on the hit logic
always @(*) begin
    if (way0_hit) begin
        data_out = way0[index];
    end else if (way1_hit) begin
        data_out = way1[index];
    end else begin
        // On a miss, or idle, default to Way 0
        // This doesn't matter since the controller ignores data on a miss
        data_out = way0[index];
    end
end
\end{lstlisting}
This combinational read behavior ensures a very low latency for cache hits. Since the read operation is asynchronous (combinational) and depends only on the stable index and hit signals from the controller, the data is available to the CPU within \textbf{2 clock cycles} of the request.
\begin{itemize}
  \item{ \textbf{Cycle 1:} The controller receives the request, checks tags, and asserts \texttt{hit} signals.}
  \item{ \textbf{Cycle 2:} The cache memory combinationally outputs the data based on the hit signals, which is then latched for the CPU.}
\end{itemize}

\subsection{Cache Controller Implementation}
The Cache Controller manages data consistency. Figure \ref{fig:cc_block} shows the top-level interface.

\begin{figure}[H]
  \centering
  % Use the renamed image
  \includegraphics[width=1.0\textwidth]{cache_controller.png}
  \caption{Cache Controller Interface Diagram.}
  \label{fig:cc_block}
\end{figure}

We implemented a robust Finite State Machine (FSM) to handle the cache lifecycle. One of the key snippet is the \textbf{Write-Through} logic. On a Write Hit, instead of invalidating the line, we update the specific cache word in-place while simultaneously sending the write to main memory.

\begin{lstlisting}[language=Verilog, caption=Write-Through with Cache Update Logic]
// Inside the S_CHECK_HIT state of the FSM
if (reg_is_write) begin
    // ** Write-Through Logic **
    if (is_hit) begin
        // Update Cache on Write Hit
        cache_mem_write_en = 1'b1;
        cache_mem_data_in  = new_cache_line; // Constructed from old line + new word
        $display("[CC] Write Update Hit: Updating Cache Index %d", addr_index);
    end
    
    // Always proceed to S_WRITE_THROUGH to update main memory
    // This ensures Main Memory is always up-to-date
    next_state = S_WRITE_THROUGH;
end
\end{lstlisting}
As shown above, if a write hit is detected (\texttt{is\_hit}), we construct a new cache line (\texttt{new\_cache\_line}) by merging the incoming word with the existing block data. We then enable writing to the cache memory. Regardless of hit or miss, the state machine transitions to \texttt{S\_WRITE\_THROUGH} to ensure the data is also written to main memory, maintaining strict coherence.

\section{Functional Verification and Analysis}

\subsection{MMU Verification}
The MMU was verified using a dedicated testbench that simulates CPU requests and Page Table Walks.

\textbf{Test Case 1: Compulsory Misses and Refills}
As shown in Figure \ref{fig:mmu_tcl1}, the simulation begins with a cold TLB. The MMU correctly asserts the stall signal upon a miss, waits for the testbench to refill the TLB, and then successfully translates the address.

\begin{figure}[H]
  \centering
  \includegraphics[width=1.0\textwidth]{mmu_tcl_test1-2_inv.png}
  \caption{MMU Unit Test: Compulsory Misses and Subsequent Hits.}
  \label{fig:mmu_tcl1}
\end{figure}

\textbf{Test Case 2: TLB Replacement Policy}
Figure \ref{fig:mmu_tcl2} demonstrates the FIFO policy. When the TLB is full (4 entries), a request for a 5th page triggers an eviction of the first entry. A subsequent request for that evicted page correctly results in a miss.

\begin{figure}[H]
  \centering
  \includegraphics[width=1.0\textwidth]{mmu_tcl_test3_inv.png}
  \caption{MMU Unit Test: TLB Replacement and Eviction Verification.}
  \label{fig:mmu_tcl2}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=1.0\textwidth]{mmu_wvform.png}
  \caption{MMU Simulation Waveform showing Stall/Refill Handshake.}
  \label{fig:mmu_wave}
\end{figure}

\subsection{Cache Controller Verification}

The Cache Controller was verified using a script–driven environment where
each instruction (e.g., \texttt{R 0x1000}, \texttt{W 0x2000 0xDEADBEEF}) is parsed
by the testbench and executed cycle–accurately.
This setup allows us to evaluate compulsory misses, conflict misses, evictions,
and write-through correctness in a controlled microarchitectural trace.

To validate the finite state machine (FSM) and the LRU-based replacement policy,
we executed a carefully crafted sequence of ten instructions:

\begin{verbatim}
R 00001000
R 00002000
W 00001000 00001000
R 00001000
R 00002000
R 11110000
R 00001111
R 11111111
\end{verbatim}

This sequence intentionally stresses:
\begin{itemize}
  \item \textbf{Compulsory (cold) misses} on first-time accesses,
  \item \textbf{Conflict misses} when two addresses map to the same set,
  \item \textbf{Evictions} under 2-way associativity,
  \item \textbf{Write Hits} and correctness of the Write-Through policy,
  \item \textbf{LRU bit update} under alternating accesses.
\end{itemize}

\vspace{0.5cm}
\subsubsection*{TCL Output: Miss/Hit Behavior and LRU Actions}

Figure~\ref{fig:cc_tcl1} shows the textual simulation log generated by the
instruction parser testbench. Each instruction is decoded and the Cache Controller
prints its internal decisions, including:
\begin{itemize}
  \item Which set the address maps to,
  \item Whether the access was a Hit or a Miss,
  \item Whether the controller decided to evict Way~0 or Way~1,
  \item When refill data is written to the cache,
  \item Which tag is stored in the selected way.
\end{itemize}

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{cc_tcl_inv.png}
  \caption{Cache Controller TCL output (Part 1).
    Shows compulsory misses for the first two reads and proper way selection.}
  \label{fig:cc_tcl1}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{cc_tcl1_inv.png}
  \caption{Cache Controller TCL output (Part 2).
    Demonstrates LRU-based evictions and correct handling of Write-Through.}
  \label{fig:cc_tcl2}
\end{figure}


\vspace{0.5cm}
\subsubsection*{Waveform-Based FSM Validation}

Figure~\ref{fig:cc_waveform} captures a snapshot of the full waveform from Synopsys Verdi.
It highlights:
\begin{itemize}
  \item Clean transitions between FSM states (\texttt{S\_CHECK\_HIT},
        \texttt{S\_READ\_MISS\_FETCH}, \texttt{S\_READ\_MISS\_WAIT}, etc.)
  \item Correct assertion of \texttt{main\_mem\_read\_req} and
        \texttt{main\_mem\_ready}
  \item Proper generation of \texttt{cache\_mem\_write\_en}
  \item Cache index selection (\texttt{cache\_index}) toggling between Sets 0 and 4
  \item Tag replacement consistent with LRU
  \item \textbf{Hit-Miss} signal alignment with refill completions
\end{itemize}

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{cc_wvfrm_inv.png}
  \caption{Cache Controller Waveform.
    Demonstrates FSM sequencing, miss handling, Write-Through behavior, and LRU-controlled way selection.}
  \label{fig:cc_waveform}
\end{figure}

\subsubsection*{Summary of Results}

Across all ten instructions, the controller exhibited:
\begin{itemize}
  \item \textbf{100\% correct hit/miss classification}
  \item \textbf{Correct Write-Through updates} to both cache and main memory
  \item \textbf{Conflict miss resolution} using 1-bit LRU
  \item \textbf{Tag and data store consistency} verified through waveform inspection
  \item \textbf{Cycle-accurate refill timing} with deterministic stalls
\end{itemize}

These results validate the correctness of the RTL, the FSM sequencing,
and the testbench-driven verification methodology.



% --- Chapter 5: Future Work ---
\chapter{Future Work}

\section{Future Objectives}
While the RTL design and functional verification are complete, the ultimate goal of this project is to proceed through the complete ASIC design flow towards tapeout. The future work is therefore focused on the physical implementation stages.

\begin{enumerate}
  \item \textbf{Logic Synthesis:} The verified RTL will be synthesized using Synopsys Design Compiler to generate a gate-level netlist mapped to a standard cell library (e.g., 45nm or 28nm technology node). This step will provide concrete area, power, and timing estimates.
  \item \textbf{Static Timing Analysis (STA):} Comprehensive timing analysis will be performed using Synopsys PrimeTime to ensure the design meets setup and hold time constraints across various process, voltage, and temperature (PVT) corners.
  \item \textbf{Floorplanning and Placement:} We will define the physical footprint of the chip, placing memory macros (SRAMs) and standard cells to optimize wire length and congestion.
  \item \textbf{Clock Tree Synthesis (CTS):} A robust clock distribution network will be inserted to minimize clock skew and insertion delay across the chip.
  \item \textbf{Routing and Physical Verification:} The final interconnections will be routed, followed by Design Rule Checking (DRC) and Layout vs. Schematic (LVS) checks to ensure the physical layout matches the logical design and manufacturing constraints.
  \item \textbf{GDSII Generation:} The final deliverable will be the GDSII stream file, ready for foundry fabrication (tapeout).
  \item \textbf{Modular Expansion:} Extend the modularity of the cache controller to support plug-and-play replacement policies (e.g., Pseudo-LRU, Random) and alternative write policies (e.g., Write-Back with Dirty Bits). This will allow for comparative benchmarking of different cache strategies within the same architectural framework.
\end{enumerate}

\pagebreak
\addcontentsline{toc}{section}{References}
\begin{thebibliography}{99}

  \bibitem{hennessy}
  J. L. Hennessy and D. A. Patterson, \textit{Computer Architecture: A Quantitative Approach}, 6th ed. Morgan Kaufmann, 2017.

  \bibitem{harris}
  D. M. Harris and S. L. Harris, \textit{Digital Design and Computer Architecture}, 2nd ed. Morgan Kaufmann, 2012.

  \bibitem{sarangi2017computer}
  S. R. Sarangi, \textit{Computer Architecture and Organization}. McGraw-Hill Education, 2017.

  \bibitem{shakti_cache}
  SHAKTI Processor Project, ``SHAKTI Cache Controller Implementation in Bluespec,'' \url{https://gitlab.com/shaktiproject/uncore/caches}.

  \bibitem{branch_edu_cache}
  Branch Education, ``Cache Memory and RAM Explained,'' \url{https://youtu.be/7J7X7aZvMXQ?si=TxehuOpF31d4Nuxu}.

  \bibitem{tech_nikola}
  Tech With Nikola, ``But, what is Virtual Memory?'', \url{https://youtu.be/A9WLYbE0p-I?si=BJN-8I-X92I3BC5-}.

  \bibitem{gfg_cache}
  Geeks for Geeks, ``Cache Memory in Computer Organization,'' \url{https://www.geeksforgeeks.org/cache-memory-in-computer-organization/}.

  \bibitem{gfg_vipt}
  Geeks for Geeks, ``Virtually Indexed Physically Tagged (VIPT) Cache,'' \url{https://www.geeksforgeeks.org/computer-organization-architecture/virtually-indexed-physically-tagged-vipt-cache/}.

\end{thebibliography}

\end{document}